# src/plugins/hai_turtle_soup/plugin.py
import os
import json
import aiohttp
from typing import List, Tuple, Type, Optional
from src.plugin_system import (
    BasePlugin,
    register_plugin,
    BaseCommand,
    ComponentInfo,
    ConfigField
)

PLUGIN_DIR = os.path.dirname(__file__)

# å…¨å±€æ¸¸æˆçŠ¶æ€å­˜å‚¨
game_states = {}  # {group_id: {"current_question": "", "current_answer": "", "hints_used": 0, "game_active": False, "guess_history": [], "game_over": False}}

@register_plugin
class HaiTurtleSoupPlugin(BasePlugin):
    plugin_name = "turtlesoup_plugin"
    plugin_description = "æ”¯æŒå…¨ç¨‹ LLM çš„æµ·é¾Ÿæ±¤æ¸¸æˆæ’ä»¶"
    plugin_version = "1.6.0"
    plugin_author = "Unreal and ä½•å¤•"
    enable_plugin = True

    dependencies = []
    python_dependencies = ["aiohttp"]

    config_file_name = "config.toml"
    config_section_descriptions = {
        "plugin": "æ’ä»¶å¯ç”¨é…ç½®",
        "llm": "LLM API é…ç½®"
    }

    config_schema = {
        "plugin": {
            "enabled": ConfigField(
                type=bool,
                default=True,
                description="æ˜¯å¦å¯ç”¨æµ·é¾Ÿæ±¤æ’ä»¶"
            ),
            "config_version": ConfigField(
                type=str,
                default="1.6.0",
                description="é…ç½®æ–‡ä»¶ç‰ˆæœ¬"
            ),
        },
        "llm": {
            "api_url": ConfigField(
                type=str,
                default="https://api.siliconflow.cn/v1/chat/completions",
                description="LLM API åœ°å€"
            ),
            "api_key": ConfigField(
                type=str,
                default="YOUR_KEY",
                description="LLM API å¯†é’¥"
            ),
            "model": ConfigField(
                type=str,
                default="gpt-3.5-turbo",
                description="ä½¿ç”¨çš„æ¨¡å‹"
            ),
            "temperature": ConfigField(
                type=float,
                default=0.7,
                description="æ–‡æœ¬ç”Ÿæˆéšæœºæ€§"
            )
        }
    }

    def get_plugin_components(self) -> List[Tuple[ComponentInfo, Type]]:
        return [
            (HaiTurtleSoupCommand.get_command_info(), HaiTurtleSoupCommand),
        ]

class HaiTurtleSoupCommand(BaseCommand):
    command_name = "HaiTurtleSoupCommand"
    command_description = "ç”Ÿæˆæµ·é¾Ÿæ±¤é¢˜ç›®æˆ–äº’åŠ¨ /hgt [é—®é¢˜|æç¤º|æ•´ç†çº¿ç´¢|æ±¤é¢|çŒœè°œ|é€€å‡º|å¸®åŠ©|æ­ç§˜]"
    command_pattern = r"^/hgt(?:\s+(?P<action>(?:æç¤º|é—®é¢˜|æ•´ç†çº¿ç´¢|çŒœè°œ|é€€å‡º|å¸®åŠ©|æ­ç§˜|æ±¤é¢)))(?:\s+(?P<rest>.+))?$"
    command_help = (
    "æµ·é¾Ÿæ±¤æ¸¸æˆ:\n"
    "/hgt é—®é¢˜(ç”Ÿæˆé¢˜ç›®)\n"
    "/hgt é—®é¢˜ è¿™é‡ŒåŠ ä¸Šä½ çš„é—®é¢˜ (å‘botæé—®)\n"
    "/hgt æç¤º (è·å–æç¤º)\n"
    "/hgt æ•´ç†çº¿ç´¢ (æ•´ç†çº¿ç´¢)\n"
    "/hgt æ±¤é¢ (æŸ¥çœ‹å½“å‰é¢˜ç›®)\n"   # ğŸ‘ˆ æ–°å¢
    "/hgt çŒœè°œ <ç­”æ¡ˆ> (çŒœæµ‹æ±¤åº•)\n"
    "/hgt é€€å‡º (ç»“æŸæ¸¸æˆ)\n"
    "/hgt æ­ç§˜ (ç›´æ¥æŸ¥çœ‹ç­”æ¡ˆå¹¶ç»“æŸæ¸¸æˆ)\n"
    "/hgt å¸®åŠ© (æŸ¥çœ‹å¸®åŠ©)"
    )

    command_examples = [
        "/hgt é—®é¢˜",
        "/hgt é—®é¢˜ ä¸ºä»€ä¹ˆæµ·é¾Ÿä¸å–æ°´ï¼Ÿ",
        "/hgt æç¤º",
        "/hgt æ•´ç†çº¿ç´¢",
        "/hgt æ±¤é¢", 
        "/hgt çŒœè°œ æµ·é¾Ÿæ˜¯ç”¨æµ·é¾Ÿåšçš„",
        "/hgt é€€å‡º",
        "/hgt æ­ç§˜",
        "/hgt å¸®åŠ©"
    ]
    intercept_message = True

    async def execute(self):
        matched_groups = self.matched_groups or {}
        action = str(matched_groups.get("action") or "").strip()
        rest_input = str(matched_groups.get("rest") or "").strip()

        chat_stream = getattr(self, 'chat_stream', None) or getattr(getattr(self, 'message', None), 'chat_stream', None)
        if chat_stream is None:
            await self.send_text("âŒ æ— æ³•è·å–èŠå¤©ä¸Šä¸‹æ–‡ä¿¡æ¯")
            return False, "ç¼ºå°‘chat_stream", True
        stream_id = getattr(chat_stream, 'stream_id', None)

        # æ£€æŸ¥æ’ä»¶æ˜¯å¦å¯ç”¨
        if not self.get_config("plugin.enabled", True):
            await self.send_text("âŒ æ’ä»¶å·²è¢«ç¦ç”¨")
            return False, "æ’ä»¶æœªå¯ç”¨", True

        # è·å– LLM é…ç½®
        api_url = self.get_config("llm.api_url", "")
        api_key = self.get_config("llm.api_key", "")
        model = self.get_config("llm.model", "gpt-3.5-turbo")
        temperature = self.get_config("llm.temperature", 0.7)

        if not api_url or not api_key:
            await self.send_text("âŒ LLM API é…ç½®ä¸å®Œæ•´")
            return False, "APIé…ç½®é”™è¯¯", True

        # è·å–ç¾¤/ç”¨æˆ· ID
        group_id = getattr(chat_stream, 'group_info', None)
        if group_id:
            group_id = group_id.group_id
        else:
            group_id = getattr(getattr(chat_stream, 'user_info', None), 'user_id', "unknown")

        # åˆå§‹åŒ–æ¸¸æˆçŠ¶æ€
        game_state = game_states.get(group_id, {})
        if group_id not in game_states:
            game_states[group_id] = game_state

        # --- åˆ†æ”¯é€»è¾‘ ---
        if action == "é—®é¢˜" and rest_input:
            return await self._handle_question(group_id, rest_input, api_url, api_key, model, temperature)
        elif action == "æç¤º":
            return await self._handle_hint(group_id, api_url, api_key, model, temperature)
        elif action == "æ•´ç†çº¿ç´¢":
            return await self._handle_clues(group_id, api_url, api_key, model, temperature)
        elif action == "æ±¤é¢":
            state = game_states.get(group_id)
            if not state.get("game_active"):
                await self.send_text("âŒ å½“å‰æ²¡æœ‰è¿›è¡Œä¸­çš„æ¸¸æˆ")
                return False, "æ— æ¸¸æˆ", True
            await self.send_text(f"ğŸ² å½“å‰æµ·é¾Ÿæ±¤é¢˜ç›®:\n{state.get('current_question')}")
            return True, "æŸ¥çœ‹æ±¤é¢", True
        elif action == "çŒœè°œ" and rest_input:
            return await self._handle_guess(group_id, rest_input, api_url, api_key, model, temperature)
        elif action == "æ­ç§˜":
            # ç”¨æˆ·è¯·æ±‚ç›´æ¥æŸ¥çœ‹ç­”æ¡ˆ
            if not game_state.get("game_active", False):
                await self.send_text("âŒ å½“å‰æ²¡æœ‰æ­£åœ¨è¿›è¡Œçš„æ¸¸æˆã€‚è¯·å…ˆä½¿ç”¨ /hgt ç”Ÿæˆé¢˜ç›®ã€‚")
                return False, "æ— æ¸¸æˆ", True

            answer = game_state.get("current_answer", "æ— ç­”æ¡ˆ")
            await self.send_text(f"ğŸ”“ å½“å‰æµ·é¾Ÿæ±¤ç­”æ¡ˆæ˜¯:\n{answer}\næ¸¸æˆç»“æŸã€‚")
            
            # æ ‡è®°æ¸¸æˆç»“æŸ
            game_state["game_active"] = False
            game_state["game_over"] = True
            game_states[group_id] = game_state  # ä¿å­˜æ›´æ–°åçš„çŠ¶æ€

            return True, "å·²æ­ç§˜", True
        elif action == "é€€å‡º":
            return await self._handle_exit(group_id)
        elif action == "å¸®åŠ©":
            await self.send_text(self.command_help)
            return True, "æ˜¾ç¤ºå¸®åŠ©", True
        else:
            return await self._start_new_game(group_id, api_url, api_key, model, temperature, stream_id)

    # --- æ¸¸æˆé€»è¾‘æ–¹æ³• ---
    async def _handle_question(self, group_id, question, api_url, api_key, model, temperature):
        state = game_states.get(group_id)
        if not state.get("game_active"):
            return await self._start_new_game(group_id, api_url, api_key, model, temperature, None)

        state.setdefault("guess_history", []).append({"type": "question", "content": question})

        prompt = f"""
ä½ æ˜¯ä¸€ä¸ªæµ·é¾Ÿæ±¤æ¸¸æˆä¸“å®¶ã€‚
å½“å‰é¢˜ç›®: {state.get('current_question')}
å½“å‰ç­”æ¡ˆ: {state.get('current_answer')}
ç”¨æˆ·æé—®: {question}
è¯·ç”¨ç®€çŸ­çš„å›ç­”å›åº”ç©å®¶ï¼Œä¸è¦é€éœ²ç­”æ¡ˆã€‚
"""
        llm_response = await self._call_llm_api(prompt, api_url, api_key, model, temperature)
        reply = llm_response.strip() or "âŒ LLMæœªè¿”å›å›ç­”"
        await self.send_text(f"â“ ä½ é—®: {question}\nğŸ’¡ å›ç­”: {reply}")
        return True, "é—®é¢˜å›ç­”å®Œæˆ", True

    async def _handle_hint(self, group_id, api_url, api_key, model, temperature):
        state = game_states.get(group_id)
        if not state.get("game_active"):
            await self.send_text("âŒ å½“å‰æ²¡æœ‰è¿›è¡Œä¸­çš„æ¸¸æˆ")
            return False, "æ— æ¸¸æˆ", True
        if state.get("hints_used", 0) >= 3:
            await self.send_text("ğŸ’¡ æç¤ºå·²ç”¨å®Œ")
            return False, "æç¤ºç”¨å°½", True

        prompt = f"""
ä½ æ˜¯ä¸€ä¸ªæµ·é¾Ÿæ±¤æ¸¸æˆä¸“å®¶ã€‚
é¢˜ç›®: {state.get('current_question')}
ç­”æ¡ˆ: {state.get('current_answer')}
è¯·æä¾›ä¸€ä¸ªä¸ç›´æ¥é€éœ²ç­”æ¡ˆçš„æç¤ºã€‚
"""
        hint = await self._call_llm_api(prompt, api_url, api_key, model, temperature)
        state["hints_used"] = state.get("hints_used", 0) + 1
        game_states[group_id] = state
        await self.send_text(f"ğŸ’¡ æç¤º ({state['hints_used']}/3): {hint.strip()}")
        return True, "æç¤ºå®Œæˆ", True

    async def _handle_clues(self, group_id, api_url, api_key, model, temperature):
        state = game_states.get(group_id)
        if not state.get("game_active"):
            await self.send_text("âŒ å½“å‰æ²¡æœ‰è¿›è¡Œä¸­çš„æ¸¸æˆ")
            return False, "æ— æ¸¸æˆ", True
        guess_history = "\n".join([item["content"] for item in state.get("guess_history", [])])
        prompt = f"""
ä½ æ˜¯ä¸€ä¸ªæµ·é¾Ÿæ±¤æ¸¸æˆä¸“å®¶ã€‚
é¢˜ç›®: {state.get('current_question')}
ç­”æ¡ˆ: {state.get('current_answer')}
è¯·æ•´ç†å…³é”®çº¿ç´¢ï¼Œç®€æ˜åˆ—å‡ºï¼Œä¸åŒ…å«ç­”æ¡ˆã€‚
å·²æœ‰è®°å½•:
{guess_history}
"""
        clues = await self._call_llm_api(prompt, api_url, api_key, model, temperature)
        await self.send_text(f"ğŸ“ çº¿ç´¢æ•´ç†:\n{clues.strip()}")
        return True, "çº¿ç´¢æ•´ç†å®Œæˆ", True

    async def _handle_guess(self, group_id, guess, api_url, api_key, model, temperature):
        state = game_states.get(group_id)
        if not state.get("game_active"):
            await self.send_text("âŒ å½“å‰æ²¡æœ‰è¿›è¡Œä¸­çš„æ¸¸æˆ")
            return False, "æ— æ¸¸æˆ", True
        if state.get("game_over"):
            await self.send_text("âŒ æ¸¸æˆå·²ç»“æŸï¼Œè¯·å¼€å§‹æ–°æ¸¸æˆ")
            return False, "æ¸¸æˆå·²ç»“æŸ", True

        prompt = f"""
ä½ æ˜¯æµ·é¾Ÿæ±¤æ¸¸æˆä¸“å®¶ã€‚
é¢˜ç›®: {state.get('current_question')}
ç­”æ¡ˆ: {state.get('current_answer')}
ç”¨æˆ·çŒœæµ‹: {guess}
è¯·ä»…å›ç­” æ˜¯/ä¸æ˜¯/æ— å…³ã€‚
"""
        llm_response = (await self._call_llm_api(prompt, api_url, api_key, model, temperature)).strip().lower()
        state.setdefault("guess_history", []).append(guess)
        game_states[group_id] = state

        if llm_response == "æ˜¯":
            state["game_over"] = True
            game_states[group_id] = state
            await self.send_text(f"ğŸ‰ çŒœå¯¹äº†ï¼ç­”æ¡ˆ: {state.get('current_answer')}")
        elif llm_response == "ä¸æ˜¯":
            await self.send_text(f"âŒ çŒœé”™äº†ï¼æç¤ºæ¬¡æ•°: {state.get('hints_used',0)}/3")
        else:
            await self.send_text("â“ ä½ çš„å›ç­”ä¸é¢˜ç›®æ— å…³")
        return True, "çŒœè°œå®Œæˆ", True

    async def _handle_exit(self, group_id):
        game_states[group_id] = {"current_question":"","current_answer":"","hints_used":0,"game_active":False,"guess_history":[],"game_over":False}
        await self.send_text("ğŸ›‘ æ¸¸æˆå·²é€€å‡º")
        return True, "é€€å‡ºæ¸¸æˆ", True

    async def _start_new_game(self, group_id, api_url, api_key, model, temperature, stream_id):
        state = game_states.get(group_id, {})

    # å¦‚æœå·²ç»æœ‰é¢˜ç›®åœ¨è¿›è¡Œä¸­ï¼Œå°±ä¸å…è®¸å†å‡ºé¢˜
        if state.get("game_active", False) and not state.get("game_over", False):
            await self.send_text("âš ï¸ å½“å‰å·²ç»æœ‰é¢˜ç›®åœ¨è¿›è¡Œä¸­ï¼Œè¯·å…ˆä½¿ç”¨ /hgt æ­ç§˜ æˆ– /hgt é€€å‡º å†å¼€å§‹æ–°é¢˜ã€‚")
            return False, "å·²æœ‰è¿›è¡Œä¸­çš„æ¸¸æˆ", True

        # ç”Ÿæˆé¢˜ç›®
        prompt_question = """
ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æµ·é¾Ÿæ±¤æ•…äº‹ç”Ÿæˆå™¨ã€‚
è¯·ç”Ÿæˆä¸€ä¸ªæ–°é¢–çš„æµ·é¾Ÿæ±¤é¢˜ç›®ï¼Œä¸è¦ä½¿ç”¨ç»å…¸é¢˜æï¼ˆæ¯”å¦‚ï¼šé¤é¦†ã€é£æœºã€å©šç¤¼ã€æµ·é¾Ÿç­‰ï¼‰ã€‚
è¦æ±‚ï¼š
1. é¢˜ç›®ç®€çŸ­ï¼Œæœ€å¥½ 25~50 å­—ã€‚
2. é¢˜ç›®å¿…é¡»æ˜¯æµ·é¾Ÿæ±¤é£æ ¼çš„æ¨ç†è°œé¢˜ï¼ŒåŒ…å«ä¸€ä¸ªçœ‹ä¼¼çŸ›ç›¾æˆ–å¥‡æ€ªçš„æƒ…å¢ƒã€‚
3. ä¸è¦åœ¨é¢˜ç›®é‡ŒåŒ…å«ç­”æ¡ˆã€‚
4. å¯ä»¥åŒ…å«çŒå¥‡å’Œææ€–å…ƒç´ (å¦‚æ€äººä¹‹ç±»çš„)ã€‚
5. é¢˜ç›®ç»“å°¾åº”è¯¥ç•™æœ‰æ‚¬å¿µï¼Œè®©äººå¥½å¥‡çœŸç›¸ã€‚
6. ç”Ÿæˆçš„é¢˜ç›®åº”è¯¥æ˜¯åŸåˆ›çš„ï¼Œä¸è¦å¤åˆ¶å·²æœ‰ä¾‹å­ã€‚

è¯·ç”Ÿæˆä¸€ä¸ªæµ·é¾Ÿæ±¤é¢˜ç›®ã€‚

å¯ä»¥å‚è€ƒçš„æµ·é¾Ÿæ±¤æ±¤é¢andæ±¤åº•ï¼ˆä»…ä¾›å‚è€ƒï¼Œå¯ä»¥å¥—æ¨¡ç‰ˆæˆ–è€…ç›´æ¥æ¬ï¼Œä½†æ˜¯ä¸¥æ ¼æŒ‰ç…§è¾“å‡ºæ ¼å¼ï¼Œä»…è¾“å‡ºæ±¤é¢ï¼‰ï¼š
1.ã€å­çš„çˆ±ã€‘
æ±¤é¢ï¼šæˆ‘çš„çˆ¶æ¯éƒ½ä¸ç†æˆ‘ï¼Œä½†æˆ‘è¿˜æ˜¯å¾ˆçˆ±ä»–ä»¬ã€‚
æ±¤åº•ï¼šå°æ—¶å€™æˆ‘æ˜¯ä¸ªå¾ˆå¬è¯çš„å­©å­ï¼Œçˆ¸çˆ¸å¦ˆå¦ˆç»å¸¸ç»™æˆ‘å¥½åƒçš„æ°´æœï¼Œæˆ‘åƒä¸å®Œã€‚ä»–ä»¬å°±å‘Šè¯‰æˆ‘å–œæ¬¢çš„ä¸œè¥¿ä¸€å®šè¦æ”¾è¿›å†°ç®±ï¼Œè¿™æ ·å¯ä»¥ä¿é²œï¼Œè®°å¾—é‚£æ—¶å€™ä»–ä»¬å·¥ä½œå¯è¾›è‹¦äº†ï¼Œç»å¸¸åŠ ç­åˆ°æ·±å¤œã€‚æ²¡ç¡è¿‡ä¸€ä¸ªå¥½è§‰ã€‚äºæ˜¯æˆ‘è€äº†ä¸ªå°èªæ˜ï¼Œåœ¨ä»–ä»¬çš„æ°´é‡Œä¸‹äº†å®‰çœ è¯ã€‚ä»–ä»¬ç¡å¾—å¯é¦™äº†ï¼Œç„¶åæˆ‘æŠŠä»–ä»¬æ”¾è¿›å†°ç®±é‡Œï¼Œä»é‚£ä»¥åæˆ‘æ¯å¤©éƒ½ä¼šå¯¹ä»–ä»¬è¯´ï¼šçˆ¸çˆ¸å¦ˆå¦ˆæˆ‘çˆ±ä½ ä»¬ã€‚ç°åœ¨æˆ‘éƒ½å…­åäº†ï¼Œä»–ä»¬è¿˜æ˜¯é‚£ä¹ˆå¹´è½»ã€‚

2.ã€èˆã€‘
æ±¤é¢ï¼šæˆ‘å…­å²é‚£å¹´ï¼Œå¤–å…¬å»ä¸–ï¼Œæˆ‘å’Œäº²äººä¸€èµ·å»ç¥­å¥ ï¼Œå’Œå§å§ç©æ‰è¿·è—ï¼Œç„¶åæˆ‘å¯¹æ¯äº²è¯´äº†å¥è¯æŠŠå¥¹å“æ˜äº†è¿‡å»ã€‚
æ±¤åº•ï¼šæˆ‘å»å‚åŠ å¤–å…¬çš„è‘¬ç¤¼ï¼ŒåŒè¡Œçš„è¿˜æœ‰æ¯”æˆ‘å¤§ä¸¤å²çš„å§å§ï¼Œæˆ‘å’Œå¥¹å®Œæ‰è¿·è—æˆ‘æ²¡æœ‰æ‰¾åˆ°å¥¹æ²¡æƒ³åˆ°å¥¹èº²åœ¨äº†çº¸åšçš„æˆ¿å­é‡Œï¼Œå½“çº¸æˆ¿å­è¢«ç‚¹ç‡ƒï¼Œæˆ‘çœ‹è§å§å§åœ¨è·³èˆï¼Œæˆ‘å¯¹å¦ˆè¯´ï¼Œå¦ˆå§å§åœ¨é‚£æˆ¿å­é‡Œé¢è·³èˆï¼Œå› ä¸ºå§å§è¢«çƒ§æ­»äº†ï¼Œæˆ‘ä¸€ç›´è®°å¾—è¿™ä¸ªäº‹ã€‚

3.ã€æ’è¿›æ¥ã€‘
æ±¤é¢ï¼šä»–è¿…é€Ÿçš„æ’è¿›æ¥ï¼Œåˆè¿…é€Ÿçš„æ‹”å‡ºå»ã€‚ååå¤å¤ï¼Œæˆ‘æµè¡€äº†ã€‚ä»–æ»¡å¤´å¤§æ±—ï¼Œéœ²å‡ºäº†ç¬‘å®¹ã€‚â€œå•Šï¼Œå¥½èˆ’æœâ€
æ±¤åº•ï¼šä»–æ˜¯æŠ¤å£«ï¼Œåœ¨ç»™æˆ‘æ‰“é’ˆï¼Œé’ˆå¤´æ‰“è¿›è¡€ç®¡é‡Œé¢ä¼šå›è¡€ï¼Œå› æ­¤è¯´æ˜æˆåŠŸäº†ã€‚æµæ±—æ˜¯å› ä¸ºååå¤å¤äº†å¥½å‡ æ¬¡ã€‚

4.ã€æ— ç½ªã€‘
æ±¤é¢ï¼š"å¥¹æ˜¯è‡ªæ„¿çš„ï¼"å°¸ä½“æ— æš´åŠ›ç—•è¿¹ï¼Œå‡¶æ‰‹è¢«åˆ¤æ— ç½ªã€‚"æˆ‘æ˜¯æ— ç½ªçš„ï¼"å°¸ä½“æœ‰æš´åŠ›ç—•è¿¹ï¼Œå‡¶æ‰‹ä¹Ÿè¢«åˆ¤æ— ç½ªã€‚
æ±¤åº•ï¼šç¬¬ä¸€å¹•ï¼šå¥³å„¿ä¸ºæ•‘ä»–äººï¼ˆå¦‚å™¨å®˜ç§»æ¤ï¼‰è‡ªæ„¿ç‰ºç‰²ï¼Œæ‰€ä»¥"è‡ªæ„¿"ä¸”æ— æš´åŠ›ç—•è¿¹ï¼Œä»–äººæ— ç½ªã€‚ç¬¬äºŒå¹•ï¼šçˆ¶äº²æ— æ³•æ¥å—å¥³å„¿æ­»äº¡çœŸç›¸ï¼Œæ€å®³äº†è¢«åˆ¤æ— ç½ªçš„äººï¼Œä½†æ³•åŒ»å‘ç°æ­¤äººæ‰€å—æš´åŠ›ä¼¤å®³ä¸çˆ¶äº²è¡Œä¸ºä¸ç¬¦ï¼ˆæˆ–çˆ¶äº²ä¼ªé€ è¯æ®ï¼‰ï¼ŒçœŸç›¸æ˜¯å¥³å„¿æ­»äºæ„å¤–ï¼Œçˆ¶äº²ä¸ºæŠ¥å¤è¯¯æ€ä»–äººï¼Œæ•…çˆ¶äº²ä¹Ÿç§°è‡ªå·±"æ— ç½ª"ï¼Œä½†æ³•å¾‹ä¸Šä»æœ‰ç½ªã€‚
"""
        question = (await self._call_llm_api(prompt_question, api_url, api_key, model, temperature)).strip()
        # ç”Ÿæˆç­”æ¡ˆ
        prompt_answer = f"""
é¢˜ç›®: {question}
ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æµ·é¾Ÿæ±¤æ•…äº‹ä¸“å®¶ã€‚è¯·ç”Ÿæˆå¯¹åº”çš„åˆç†çš„ç­”æ¡ˆï¼Œå¯ä»¥è•´å«ææ€–å…ƒç´ ï¼ˆæ¯”å¦‚æ€äººä¹‹ç±»çš„ï¼‰ï¼Œè®²ç©¶é€»è¾‘å’Œä¸€äº›ç°å®ï¼Œä¸è¦è§£é‡Šã€‚150å­—ä»¥å†…ã€‚

å¯ä»¥å‚è€ƒçš„æµ·é¾Ÿæ±¤æ±¤é¢andæ±¤åº•ï¼ˆä»…ä¾›å‚è€ƒï¼Œå¯ä»¥å¥—æ¨¡ç‰ˆæˆ–è€…ç›´æ¥æ¬ï¼Œä½†æ˜¯ä¸¥æ ¼æŒ‰ç…§è¾“å‡ºæ ¼å¼ï¼Œä»…è¾“å‡ºæ±¤åº•ï¼‰ï¼š
1.ã€å­çš„çˆ±ã€‘
æ±¤é¢ï¼šæˆ‘çš„çˆ¶æ¯éƒ½ä¸ç†æˆ‘ï¼Œä½†æˆ‘è¿˜æ˜¯å¾ˆçˆ±ä»–ä»¬ã€‚
æ±¤åº•ï¼šå°æ—¶å€™æˆ‘æ˜¯ä¸ªå¾ˆå¬è¯çš„å­©å­ï¼Œçˆ¸çˆ¸å¦ˆå¦ˆç»å¸¸ç»™æˆ‘å¥½åƒçš„æ°´æœï¼Œæˆ‘åƒä¸å®Œã€‚ä»–ä»¬å°±å‘Šè¯‰æˆ‘å–œæ¬¢çš„ä¸œè¥¿ä¸€å®šè¦æ”¾è¿›å†°ç®±ï¼Œè¿™æ ·å¯ä»¥ä¿é²œï¼Œè®°å¾—é‚£æ—¶å€™ä»–ä»¬å·¥ä½œå¯è¾›è‹¦äº†ï¼Œç»å¸¸åŠ ç­åˆ°æ·±å¤œã€‚æ²¡ç¡è¿‡ä¸€ä¸ªå¥½è§‰ã€‚äºæ˜¯æˆ‘è€äº†ä¸ªå°èªæ˜ï¼Œåœ¨ä»–ä»¬çš„æ°´é‡Œä¸‹äº†å®‰çœ è¯ã€‚ä»–ä»¬ç¡å¾—å¯é¦™äº†ï¼Œç„¶åæˆ‘æŠŠä»–ä»¬æ”¾è¿›å†°ç®±é‡Œï¼Œä»é‚£ä»¥åæˆ‘æ¯å¤©éƒ½ä¼šå¯¹ä»–ä»¬è¯´ï¼šçˆ¸çˆ¸å¦ˆå¦ˆæˆ‘çˆ±ä½ ä»¬ã€‚ç°åœ¨æˆ‘éƒ½å…­åäº†ï¼Œä»–ä»¬è¿˜æ˜¯é‚£ä¹ˆå¹´è½»ã€‚

2.ã€èˆã€‘
æ±¤é¢ï¼šæˆ‘å…­å²é‚£å¹´ï¼Œå¤–å…¬å»ä¸–ï¼Œæˆ‘å’Œäº²äººä¸€èµ·å»ç¥­å¥ ï¼Œå’Œå§å§ç©æ‰è¿·è—ï¼Œç„¶åæˆ‘å¯¹æ¯äº²è¯´äº†å¥è¯æŠŠå¥¹å“æ˜äº†è¿‡å»ã€‚
æ±¤åº•ï¼šæˆ‘å»å‚åŠ å¤–å…¬çš„è‘¬ç¤¼ï¼ŒåŒè¡Œçš„è¿˜æœ‰æ¯”æˆ‘å¤§ä¸¤å²çš„å§å§ï¼Œæˆ‘å’Œå¥¹å®Œæ‰è¿·è—æˆ‘æ²¡æœ‰æ‰¾åˆ°å¥¹æ²¡æƒ³åˆ°å¥¹èº²åœ¨äº†çº¸åšçš„æˆ¿å­é‡Œï¼Œå½“çº¸æˆ¿å­è¢«ç‚¹ç‡ƒï¼Œæˆ‘çœ‹è§å§å§åœ¨è·³èˆï¼Œæˆ‘å¯¹å¦ˆè¯´ï¼Œå¦ˆå§å§åœ¨é‚£æˆ¿å­é‡Œé¢è·³èˆï¼Œå› ä¸ºå§å§è¢«çƒ§æ­»äº†ï¼Œæˆ‘ä¸€ç›´è®°å¾—è¿™ä¸ªäº‹ã€‚

3.ã€æ’è¿›æ¥ã€‘
æ±¤é¢ï¼šä»–è¿…é€Ÿçš„æ’è¿›æ¥ï¼Œåˆè¿…é€Ÿçš„æ‹”å‡ºå»ã€‚ååå¤å¤ï¼Œæˆ‘æµè¡€äº†ã€‚ä»–æ»¡å¤´å¤§æ±—ï¼Œéœ²å‡ºäº†ç¬‘å®¹ã€‚"å•Šï¼Œå¥½èˆ’æœ"
æ±¤åº•ï¼šä»–æ˜¯å®ä¹ æŠ¤å£«ï¼Œåœ¨ç»™æˆ‘æ‰“é’ˆï¼Œé’ˆå¤´æ‰“è¿›è¡€ç®¡é‡Œé¢ä¼šå›è¡€ï¼Œå› æ­¤è¯´æ˜æˆåŠŸäº†ã€‚æµæ±—æ˜¯å› ä¸ºååå¤å¤äº†å¥½å‡ æ¬¡ï¼Œè®©äººç´§å¼ ã€‚

4.ã€æ— ç½ªã€‘
æ±¤é¢ï¼š"å¥¹æ˜¯è‡ªæ„¿çš„ï¼"å°¸ä½“æ— æš´åŠ›ç—•è¿¹ï¼Œå‡¶æ‰‹è¢«åˆ¤æ— ç½ªã€‚"æˆ‘æ˜¯æ— ç½ªçš„ï¼"å°¸ä½“æœ‰æš´åŠ›ç—•è¿¹ï¼Œå‡¶æ‰‹ä¹Ÿè¢«åˆ¤æ— ç½ªã€‚
æ±¤åº•ï¼šç¬¬ä¸€å¹•ï¼šå¥³å„¿ä¸ºæ•‘ä»–äººï¼ˆå¦‚å™¨å®˜ç§»æ¤ï¼‰è‡ªæ„¿ç‰ºç‰²ï¼Œæ‰€ä»¥"è‡ªæ„¿"ä¸”æ— æš´åŠ›ç—•è¿¹ï¼Œä»–äººæ— ç½ªã€‚ç¬¬äºŒå¹•ï¼šçˆ¶äº²æ— æ³•æ¥å—å¥³å„¿æ­»äº¡çœŸç›¸ï¼Œæ€å®³äº†è¢«åˆ¤æ— ç½ªçš„äººï¼Œä½†æ³•åŒ»å‘ç°æ­¤äººæ‰€å—æš´åŠ›ä¼¤å®³ä¸çˆ¶äº²è¡Œä¸ºä¸ç¬¦ï¼ˆæˆ–çˆ¶äº²ä¼ªé€ è¯æ®ï¼‰ï¼ŒçœŸç›¸æ˜¯å¥³å„¿æ­»äºæ„å¤–ï¼Œçˆ¶äº²ä¸ºæŠ¥å¤è¯¯æ€ä»–äººï¼Œæ•…çˆ¶äº²ä¹Ÿç§°è‡ªå·±"æ— ç½ª"ï¼Œä½†æ³•å¾‹ä¸Šä»æœ‰ç½ªã€‚
"""
        answer = (await self._call_llm_api(prompt_answer, api_url, api_key, model, temperature)).strip()
        game_states[group_id] = {"current_question": question, "current_answer": answer, "hints_used":0, "game_active":True, "guess_history":[], "game_over":False}
        await self.send_text(f"ğŸ¤” æµ·é¾Ÿæ±¤é¢˜ç›®:\n{question}\nğŸ’¡ æç¤ºæ¬¡æ•°: 0/3\nğŸ’¡ ä½¿ç”¨ /hgt é—®é¢˜ <é—®é¢˜> æé—®ï¼Œ/hgt æç¤º è·å–æç¤ºï¼Œ/hgt çŒœè°œ <ç­”æ¡ˆ> çŒœæµ‹æ±¤åº•")
        return True, "æ–°é¢˜ç›®ç”Ÿæˆå®Œæˆ", True

    async def _call_llm_api(self, prompt, api_url, api_key, model, temperature):
        headers = {"Content-Type": "application/json","Authorization": f"Bearer {api_key}"}
        payload = {"model": model,"messages":[{"role":"system","content":"ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šæµ·é¾Ÿæ±¤æ•…äº‹ç”Ÿæˆå™¨å’Œè§£é‡Šè€…ã€‚"},{"role":"user","content":prompt}],"temperature":temperature,"max_tokens":500,"stream":False}
        try:
            timeout = aiohttp.ClientTimeout(total=30)
            async with aiohttp.ClientSession(timeout=timeout) as session:
                async with session.post(api_url, headers=headers, json=payload) as resp:
                    if resp.status == 200:
                        data = await resp.json()
                        return data.get("choices",[{}])[0].get("message",{}).get("content","").strip()
                    else:
                        return ""
        except Exception as e:
            print(f"LLM APIå¼‚å¸¸: {e}")
            return ""
